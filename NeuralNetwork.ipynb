{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.utils import np_utils\n",
    "from __future__ import absolute_import, print_function, division\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load traning data\n",
    "\n",
    "def load_data():\n",
    "    with open('train.pkl', 'rb') as f:\n",
    "        return pkl.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=load_data()\n",
    "\n",
    "X = data[0]  #contains   30134 X 3136 -inputs  \n",
    "Y = data[1]  #contains   30134 X 1 - outputs\n",
    "\n",
    "\n",
    "# Split the data - i suggest ratio of data train : validation (test) to be 4 : 1 or something similar - imo \n",
    "# the bigger sample the lower ratio\n",
    "\n",
    "\n",
    "# I haven't used whole dataset but w/e it's still enough\n",
    "\n",
    "xval = X[23000:30000] \n",
    "yval = Y[23000:30000]\n",
    "\n",
    "\n",
    "\n",
    "xtrain = X[0:23000]\n",
    "ytrain = Y[0:23000]\n",
    "\n",
    "\n",
    "# Outputs must be vectors not Dx1 arrays\n",
    "\n",
    "ytrain=ytrain[:,0]\n",
    "yval=yval[:,0]\n",
    "\n",
    "\n",
    "\n",
    "# We have outputs as class number but we have to adapt data to our model so we're just creating\n",
    "# class value (e.g. 11) -----> vector of inputs[0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0 .....] 36x1\n",
    "\n",
    "\n",
    "# Im pretty sure that where is way more efficient and nicer way to do that - mby I'll google it later\n",
    "yvalBinary = np.zeros(shape=(yval.shape[0], 36))\n",
    "for i in range(yval.shape[0]):\n",
    "    yvalBinary[i][yval[i]] = 1\n",
    "\n",
    "ytrainBinary = np.zeros(shape=(ytrain.shape[0], 36))\n",
    "for i in range(ytrain.shape[0]):\n",
    "    ytrainBinary[i][ytrain[i]] = 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/120\n",
      "23000/23000 [==============================] - 4s 188us/step - loss: 2.2608 - acc: 0.4098\n",
      "Epoch 2/120\n",
      "23000/23000 [==============================] - 3s 126us/step - loss: 1.3046 - acc: 0.6496\n",
      "Epoch 3/120\n",
      "23000/23000 [==============================] - 3s 125us/step - loss: 1.0099 - acc: 0.7160\n",
      "Epoch 4/120\n",
      "23000/23000 [==============================] - 3s 128us/step - loss: 0.8755 - acc: 0.7477\n",
      "Epoch 5/120\n",
      "23000/23000 [==============================] - 3s 123us/step - loss: 0.7905 - acc: 0.7680\n",
      "Epoch 6/120\n",
      "23000/23000 [==============================] - 3s 134us/step - loss: 0.7311 - acc: 0.7841\n",
      "Epoch 7/120\n",
      "23000/23000 [==============================] - 3s 120us/step - loss: 0.6831 - acc: 0.7977\n",
      "Epoch 8/120\n",
      "23000/23000 [==============================] - 3s 118us/step - loss: 0.6453 - acc: 0.8078\n",
      "Epoch 9/120\n",
      "23000/23000 [==============================] - 3s 118us/step - loss: 0.6154 - acc: 0.8174\n",
      "Epoch 10/120\n",
      "23000/23000 [==============================] - 3s 118us/step - loss: 0.5883 - acc: 0.8228\n",
      "Epoch 11/120\n",
      "23000/23000 [==============================] - 3s 118us/step - loss: 0.5613 - acc: 0.8325\n",
      "Epoch 12/120\n",
      "23000/23000 [==============================] - 3s 122us/step - loss: 0.5331 - acc: 0.8402\n",
      "Epoch 13/120\n",
      "23000/23000 [==============================] - 3s 118us/step - loss: 0.5200 - acc: 0.8429\n",
      "Epoch 14/120\n",
      "23000/23000 [==============================] - 3s 119us/step - loss: 0.4947 - acc: 0.8519\n",
      "Epoch 15/120\n",
      "23000/23000 [==============================] - 3s 118us/step - loss: 0.4795 - acc: 0.8543\n",
      "Epoch 16/120\n",
      "23000/23000 [==============================] - 3s 118us/step - loss: 0.4605 - acc: 0.8619\n",
      "Epoch 17/120\n",
      "23000/23000 [==============================] - 3s 118us/step - loss: 0.4448 - acc: 0.8648\n",
      "Epoch 18/120\n",
      "23000/23000 [==============================] - 3s 122us/step - loss: 0.4296 - acc: 0.8695\n",
      "Epoch 19/120\n",
      "23000/23000 [==============================] - 3s 118us/step - loss: 0.4156 - acc: 0.8735\n",
      "Epoch 20/120\n",
      "23000/23000 [==============================] - 3s 118us/step - loss: 0.4002 - acc: 0.8764\n",
      "Epoch 21/120\n",
      "23000/23000 [==============================] - 3s 120us/step - loss: 0.3882 - acc: 0.8794\n",
      "Epoch 22/120\n",
      "23000/23000 [==============================] - 3s 118us/step - loss: 0.3759 - acc: 0.8847\n",
      "Epoch 23/120\n",
      "23000/23000 [==============================] - 3s 118us/step - loss: 0.3633 - acc: 0.8884\n",
      "Epoch 24/120\n",
      "23000/23000 [==============================] - 3s 121us/step - loss: 0.3526 - acc: 0.8916\n",
      "Epoch 25/120\n",
      "23000/23000 [==============================] - 3s 118us/step - loss: 0.3440 - acc: 0.8943\n",
      "Epoch 26/120\n",
      "23000/23000 [==============================] - 3s 118us/step - loss: 0.3327 - acc: 0.8967\n",
      "Epoch 27/120\n",
      "23000/23000 [==============================] - 3s 117us/step - loss: 0.3247 - acc: 0.8992\n",
      "Epoch 28/120\n",
      "23000/23000 [==============================] - 3s 118us/step - loss: 0.3200 - acc: 0.8980\n",
      "Epoch 29/120\n",
      "23000/23000 [==============================] - 3s 118us/step - loss: 0.3063 - acc: 0.9053\n",
      "Epoch 30/120\n",
      "23000/23000 [==============================] - 3s 122us/step - loss: 0.2987 - acc: 0.9068\n",
      "Epoch 31/120\n",
      "23000/23000 [==============================] - 3s 119us/step - loss: 0.2894 - acc: 0.9117\n",
      "Epoch 32/120\n",
      "23000/23000 [==============================] - 3s 119us/step - loss: 0.2825 - acc: 0.9123\n",
      "Epoch 33/120\n",
      "23000/23000 [==============================] - 3s 118us/step - loss: 0.2732 - acc: 0.9150\n",
      "Epoch 34/120\n",
      "23000/23000 [==============================] - 3s 118us/step - loss: 0.2680 - acc: 0.9144\n",
      "Epoch 35/120\n",
      "23000/23000 [==============================] - 3s 120us/step - loss: 0.2641 - acc: 0.9164\n",
      "Epoch 36/120\n",
      "23000/23000 [==============================] - 3s 119us/step - loss: 0.2588 - acc: 0.9188\n",
      "Epoch 37/120\n",
      "23000/23000 [==============================] - 3s 118us/step - loss: 0.2462 - acc: 0.9226\n",
      "Epoch 38/120\n",
      "23000/23000 [==============================] - 3s 120us/step - loss: 0.2404 - acc: 0.9247\n",
      "Epoch 39/120\n",
      "23000/23000 [==============================] - 3s 119us/step - loss: 0.2337 - acc: 0.9262\n",
      "Epoch 40/120\n",
      "23000/23000 [==============================] - 3s 118us/step - loss: 0.2299 - acc: 0.9269\n",
      "Epoch 41/120\n",
      "23000/23000 [==============================] - 3s 121us/step - loss: 0.2219 - acc: 0.9289\n",
      "Epoch 42/120\n",
      "23000/23000 [==============================] - 3s 118us/step - loss: 0.2183 - acc: 0.9295\n",
      "Epoch 43/120\n",
      "23000/23000 [==============================] - 3s 118us/step - loss: 0.2111 - acc: 0.9343\n",
      "Epoch 44/120\n",
      "23000/23000 [==============================] - 3s 118us/step - loss: 0.2086 - acc: 0.9333\n",
      "Epoch 45/120\n",
      "23000/23000 [==============================] - 3s 117us/step - loss: 0.1978 - acc: 0.9380\n",
      "Epoch 46/120\n",
      "23000/23000 [==============================] - 3s 118us/step - loss: 0.1956 - acc: 0.9396\n",
      "Epoch 47/120\n",
      "23000/23000 [==============================] - 3s 121us/step - loss: 0.1895 - acc: 0.9402\n",
      "Epoch 48/120\n",
      "23000/23000 [==============================] - 3s 119us/step - loss: 0.1854 - acc: 0.9420\n",
      "Epoch 49/120\n",
      "23000/23000 [==============================] - 3s 120us/step - loss: 0.1814 - acc: 0.9433\n",
      "Epoch 50/120\n",
      "23000/23000 [==============================] - 3s 119us/step - loss: 0.1779 - acc: 0.9437\n",
      "Epoch 51/120\n",
      "23000/23000 [==============================] - 3s 129us/step - loss: 0.1783 - acc: 0.9433\n",
      "Epoch 52/120\n",
      "23000/23000 [==============================] - 3s 132us/step - loss: 0.1671 - acc: 0.9460\n",
      "Epoch 53/120\n",
      "23000/23000 [==============================] - 3s 136us/step - loss: 0.1634 - acc: 0.9483\n",
      "Epoch 54/120\n",
      "23000/23000 [==============================] - 3s 124us/step - loss: 0.1623 - acc: 0.9470\n",
      "Epoch 55/120\n",
      "23000/23000 [==============================] - 3s 119us/step - loss: 0.1553 - acc: 0.9523\n",
      "Epoch 56/120\n",
      "23000/23000 [==============================] - 3s 120us/step - loss: 0.1527 - acc: 0.9525\n",
      "Epoch 57/120\n",
      "23000/23000 [==============================] - 3s 121us/step - loss: 0.1515 - acc: 0.9525\n",
      "Epoch 58/120\n",
      "23000/23000 [==============================] - 3s 125us/step - loss: 0.1431 - acc: 0.9558\n",
      "Epoch 59/120\n",
      "23000/23000 [==============================] - 3s 132us/step - loss: 0.1395 - acc: 0.9577\n",
      "Epoch 60/120\n",
      "23000/23000 [==============================] - 3s 128us/step - loss: 0.1377 - acc: 0.9588\n",
      "Epoch 61/120\n",
      "23000/23000 [==============================] - 3s 131us/step - loss: 0.1360 - acc: 0.9581\n",
      "Epoch 62/120\n",
      "23000/23000 [==============================] - 3s 125us/step - loss: 0.1319 - acc: 0.9588\n",
      "Epoch 63/120\n",
      "23000/23000 [==============================] - 3s 121us/step - loss: 0.1284 - acc: 0.9594\n",
      "Epoch 64/120\n",
      "23000/23000 [==============================] - 3s 126us/step - loss: 0.1266 - acc: 0.9603\n",
      "Epoch 65/120\n",
      "23000/23000 [==============================] - 3s 117us/step - loss: 0.1201 - acc: 0.9626\n",
      "Epoch 66/120\n",
      "23000/23000 [==============================] - 3s 124us/step - loss: 0.1199 - acc: 0.9614\n",
      "Epoch 67/120\n",
      "23000/23000 [==============================] - 3s 121us/step - loss: 0.1146 - acc: 0.9644\n",
      "Epoch 68/120\n",
      "23000/23000 [==============================] - 3s 116us/step - loss: 0.1140 - acc: 0.9646\n",
      "Epoch 69/120\n",
      "23000/23000 [==============================] - 3s 121us/step - loss: 0.1154 - acc: 0.9649\n",
      "Epoch 70/120\n",
      "23000/23000 [==============================] - 3s 128us/step - loss: 0.1112 - acc: 0.9647\n",
      "Epoch 71/120\n",
      "23000/23000 [==============================] - 3s 122us/step - loss: 0.1056 - acc: 0.9683\n",
      "Epoch 72/120\n",
      "23000/23000 [==============================] - 3s 131us/step - loss: 0.1031 - acc: 0.9690\n",
      "Epoch 73/120\n",
      "23000/23000 [==============================] - 3s 132us/step - loss: 0.1008 - acc: 0.9704\n",
      "Epoch 74/120\n",
      "23000/23000 [==============================] - 3s 126us/step - loss: 0.0984 - acc: 0.9692\n",
      "Epoch 75/120\n",
      "23000/23000 [==============================] - 3s 122us/step - loss: 0.0965 - acc: 0.9708\n",
      "Epoch 76/120\n",
      "23000/23000 [==============================] - 3s 115us/step - loss: 0.0941 - acc: 0.9721\n",
      "Epoch 77/120\n",
      "23000/23000 [==============================] - 3s 119us/step - loss: 0.0914 - acc: 0.9728\n",
      "Epoch 78/120\n",
      "23000/23000 [==============================] - 3s 129us/step - loss: 0.0881 - acc: 0.9737\n",
      "Epoch 79/120\n",
      "23000/23000 [==============================] - 3s 127us/step - loss: 0.0882 - acc: 0.9741\n",
      "Epoch 80/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23000/23000 [==============================] - 3s 123us/step - loss: 0.0851 - acc: 0.9747\n",
      "Epoch 81/120\n",
      "23000/23000 [==============================] - 3s 126us/step - loss: 0.0845 - acc: 0.9750\n",
      "Epoch 82/120\n",
      "23000/23000 [==============================] - 3s 124us/step - loss: 0.0817 - acc: 0.9760\n",
      "Epoch 83/120\n",
      "23000/23000 [==============================] - 3s 124us/step - loss: 0.0813 - acc: 0.9763\n",
      "Epoch 84/120\n",
      "23000/23000 [==============================] - 3s 120us/step - loss: 0.0777 - acc: 0.9772\n",
      "Epoch 85/120\n",
      "23000/23000 [==============================] - 3s 121us/step - loss: 0.0779 - acc: 0.9777\n",
      "Epoch 86/120\n",
      "23000/23000 [==============================] - 3s 125us/step - loss: 0.0745 - acc: 0.9788\n",
      "Epoch 87/120\n",
      "23000/23000 [==============================] - 3s 123us/step - loss: 0.0729 - acc: 0.9788\n",
      "Epoch 88/120\n",
      "23000/23000 [==============================] - 3s 123us/step - loss: 0.0730 - acc: 0.9777\n",
      "Epoch 89/120\n",
      "23000/23000 [==============================] - 3s 124us/step - loss: 0.0699 - acc: 0.9799\n",
      "Epoch 90/120\n",
      "23000/23000 [==============================] - 3s 124us/step - loss: 0.0695 - acc: 0.9806\n",
      "Epoch 91/120\n",
      "23000/23000 [==============================] - 3s 120us/step - loss: 0.0683 - acc: 0.9795\n",
      "Epoch 92/120\n",
      "23000/23000 [==============================] - 3s 119us/step - loss: 0.0677 - acc: 0.9801\n",
      "Epoch 93/120\n",
      "23000/23000 [==============================] - 3s 116us/step - loss: 0.0636 - acc: 0.9820\n",
      "Epoch 94/120\n",
      "23000/23000 [==============================] - 3s 116us/step - loss: 0.0628 - acc: 0.9825\n",
      "Epoch 95/120\n",
      "23000/23000 [==============================] - 3s 116us/step - loss: 0.0623 - acc: 0.9816\n",
      "Epoch 96/120\n",
      "23000/23000 [==============================] - 3s 114us/step - loss: 0.0615 - acc: 0.9815\n",
      "Epoch 97/120\n",
      "23000/23000 [==============================] - 3s 113us/step - loss: 0.0597 - acc: 0.9830\n",
      "Epoch 98/120\n",
      "23000/23000 [==============================] - 3s 119us/step - loss: 0.0587 - acc: 0.9830\n",
      "Epoch 99/120\n",
      "23000/23000 [==============================] - 3s 117us/step - loss: 0.0577 - acc: 0.9838\n",
      "Epoch 100/120\n",
      "23000/23000 [==============================] - 3s 121us/step - loss: 0.0561 - acc: 0.9837\n",
      "Epoch 101/120\n",
      "23000/23000 [==============================] - 3s 114us/step - loss: 0.0534 - acc: 0.9856\n",
      "Epoch 102/120\n",
      "23000/23000 [==============================] - 3s 117us/step - loss: 0.0533 - acc: 0.9853\n",
      "Epoch 103/120\n",
      "23000/23000 [==============================] - 3s 120us/step - loss: 0.0543 - acc: 0.9849\n",
      "Epoch 104/120\n",
      "23000/23000 [==============================] - 3s 132us/step - loss: 0.0519 - acc: 0.9852\n",
      "Epoch 105/120\n",
      "23000/23000 [==============================] - 3s 124us/step - loss: 0.0520 - acc: 0.9848\n",
      "Epoch 106/120\n",
      "23000/23000 [==============================] - 3s 121us/step - loss: 0.0492 - acc: 0.9866\n",
      "Epoch 107/120\n",
      "23000/23000 [==============================] - 3s 125us/step - loss: 0.0493 - acc: 0.9861\n",
      "Epoch 108/120\n",
      "23000/23000 [==============================] - 3s 124us/step - loss: 0.0502 - acc: 0.9847\n",
      "Epoch 109/120\n",
      "23000/23000 [==============================] - 3s 124us/step - loss: 0.0468 - acc: 0.9868\n",
      "Epoch 110/120\n",
      "23000/23000 [==============================] - 3s 130us/step - loss: 0.0459 - acc: 0.9866\n",
      "Epoch 111/120\n",
      "23000/23000 [==============================] - 3s 124us/step - loss: 0.0457 - acc: 0.9873\n",
      "Epoch 112/120\n",
      "23000/23000 [==============================] - 3s 125us/step - loss: 0.0446 - acc: 0.9882\n",
      "Epoch 113/120\n",
      "23000/23000 [==============================] - 3s 122us/step - loss: 0.0445 - acc: 0.9875\n",
      "Epoch 114/120\n",
      "23000/23000 [==============================] - 3s 125us/step - loss: 0.0431 - acc: 0.9887\n",
      "Epoch 115/120\n",
      "23000/23000 [==============================] - 3s 128us/step - loss: 0.0449 - acc: 0.9871\n",
      "Epoch 116/120\n",
      "23000/23000 [==============================] - 3s 122us/step - loss: 0.0420 - acc: 0.9885\n",
      "Epoch 117/120\n",
      "23000/23000 [==============================] - 3s 125us/step - loss: 0.0416 - acc: 0.9887\n",
      "Epoch 118/120\n",
      "23000/23000 [==============================] - 3s 123us/step - loss: 0.0397 - acc: 0.9891\n",
      "Epoch 119/120\n",
      "23000/23000 [==============================] - 3s 124us/step - loss: 0.0392 - acc: 0.9890\n",
      "Epoch 120/120\n",
      "23000/23000 [==============================] - 3s 124us/step - loss: 0.0393 - acc: 0.9893\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x218a38a4550>"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#I think everythink it clear here\n",
    "\n",
    "model1 = Sequential()\n",
    "model1.add(Dense(units=90, activation='relu', input_dim=3136))\n",
    "model1.add(Dense(units=60, activation='relu'))\n",
    "model1.add(Dense(units=36, activation='softmax'))\n",
    "\n",
    "model1.compile(loss='categorical_crossentropy',\n",
    "              optimizer='sgd',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model1.fit(xtrain, ytrainBinary, epochs=120, batch_size=50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "878\n"
     ]
    }
   ],
   "source": [
    "#Simple check how we're doing with our keras model - remember we're printing number of cases which were wrong\n",
    "kerasmodelPred=model1.predict(xval)\n",
    "\n",
    "sum=0\n",
    "for i in range (xval.shape[0]):\n",
    "    if(np.argmax(kerasmodelPred[i])!=yval[i]):\n",
    "        sum+=1\n",
    "print(sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get all parameters of the model - if u have more layers you've to add variables\n",
    "\n",
    "w1=np.asarray(model1.layers[0].get_weights()[0])\n",
    "bias1=model1.layers[0].get_weights()[1]\n",
    "w2=np.asarray(model1.layers[1].get_weights()[0])\n",
    "bias2=model1.layers[1].get_weights()[1]\n",
    "w3=np.asarray(model1.layers[2].get_weights()[0])\n",
    "bias3=model1.layers[2].get_weights()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function used by our model\n",
    "\n",
    "def relu(x):\n",
    "    return x * (x > 0)\n",
    "    \n",
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum(axis=0)\n",
    "\n",
    "    \n",
    "    \n",
    "# Here is an original Keras function implementatnion K- keras  T-theano\n",
    "# As for softmax - don't bother yourself with searching. It it use way to many subfunctions and all in all comes down to C++\n",
    "\n",
    "def reluT(x, alpha=0):\n",
    "\n",
    "    if alpha == 0:\n",
    "        return 0.5 * (x + abs(x))\n",
    "    else:\n",
    "        alpha = tensor.as_tensor_variable(alpha)\n",
    "        f1 = 0.5 * (1 + alpha)\n",
    "        f2 = 0.5 * (1 - alpha)\n",
    "        return f1 * x + f2 * abs(x)\n",
    "    \n",
    "def reluK(x, alpha=0.):\n",
    "\n",
    "    x = reluT(x, alpha)\n",
    "    return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we are making our forward pass without keras\n",
    "\n",
    "def predictWithGivenParams(bias1,bias2,bias3,w1,w2,w3,x):\n",
    "        return relu((relu((relu((x@w1) + bias1)@w2)+bias2)@w3)+bias3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "878\n"
     ]
    }
   ],
   "source": [
    "# Making the same test for matrix model\n",
    "matrixmodelPred=predictWithGivenParams(bias1,bias2,bias3,w1,w2,w3,xval)\n",
    "\n",
    "sum=0\n",
    "for i in range (xval.shape[0]):\n",
    "    if(np.argmax(matrixmodelPred[i])!=yval[i]):\n",
    "        sum+=1\n",
    "print(sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here is just example how to load weights to another model - you can ignore next 2 cells, just additional content\n",
    "\n",
    "model2 = Sequential()\n",
    "model2.add(Dense(units=90, activation='relu', input_dim=3136))\n",
    "model2.add(Dense(units=36, activation='softmax'))\n",
    "\n",
    "model2.compile(loss='categorical_crossentropy',\n",
    "              optimizer='sgd',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model2.set_weights(model1.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kerasmodel2=model2.predict(xval)\n",
    "\n",
    "sum=0\n",
    "for i in range (xval.shape[0]):\n",
    "    if(np.argmax(kerasmodel2[i])!=yval[i]):\n",
    "        sum+=1\n",
    "print(sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save your parameters as pickle files - Way better that saving to .txt, although you can also do that\n",
    "\n",
    "with open('w1', 'wb') as fp:\n",
    "    pkl.dump(w1, fp)\n",
    "    \n",
    "with open('w2', 'wb') as fp:\n",
    "    pkl.dump(w2, fp)\n",
    "    \n",
    "with open('w3', 'wb') as fp:\n",
    "    pkl.dump(w3, fp)\n",
    "\n",
    "with open('bias1', 'wb') as fp:\n",
    "    pkl.dump(bias1, fp)\n",
    "    \n",
    "with open('bias2', 'wb') as fp:\n",
    "    pkl.dump(bias2, fp)\n",
    "    \n",
    "with open('bias3', 'wb') as fp:\n",
    "    pkl.dump(bias3, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now you can just load your files and use it with our matrix model so that you won't need to import keras to run your neural network\n",
    "    with open ('filename.pkl', 'rb') as fp:\n",
    "        param = pkl.load(fp)\n",
    "\n",
    "# Mby I'll add example later on but now I cannot do that because of the fact that it's uni assignment and I can't give my whole code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
